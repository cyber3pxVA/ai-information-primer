<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>AI Education Hub - Specialty Care Medicine</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700&display=swap" rel="stylesheet">
    <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/6.0.0/css/all.min.css">
</head>
<body>
    <header class="header">
        <nav class="nav">
            <div class="nav-brand">
                <i class="fas fa-brain"></i>
                <h1>AI Education Hub</h1>
            </div>
            <ul class="nav-menu">
                <li><a href="#home" class="nav-link">Home</a></li>
                <li><a href="#articles" class="nav-link">Research Articles</a></li>
                <li><a href="#resources" class="nav-link">Downloads</a></li>
                <li><a href="#repository" class="nav-link">Repository</a></li>
                <li><a href="#contact" class="nav-link">Contact</a></li>
            </ul>
            <div class="hamburger">
                <span></span>
                <span></span>
                <span></span>
            </div>
        </nav>
    </header>

    <main>
        <!-- Hero Section -->
        <section id="home" class="hero">
            <div class="container">
                <div class="hero-content">
                    <h1 class="hero-title">AI Information Primer</h1>
                    <p class="hero-subtitle">Specialty Care Medicine - New England Healthcare System</p>
                    <p class="hero-description">
                        Comprehensive collection of AI research articles, educational resources, and key advances 
                        in artificial intelligence. Explore the latest developments in LLMs, transformers, and AI agents.
                    </p>
                    <div class="hero-buttons">
                        <a href="#articles" class="btn btn-primary">Explore Articles</a>
                        <a href="https://github.com/cyber3pxVA/ai-information-primer" target="_blank" class="btn btn-secondary">
                            <i class="fab fa-github"></i> View Repository
                        </a>
                    </div>
                </div>
            </div>
        </section>

        <!-- Research Articles Section -->
        <section id="articles" class="articles-section">
            <div class="container">
                <h2 class="section-title">Major Themes & Key Advances in AI</h2>
                <p class="section-subtitle">Essential research papers that shaped modern AI development</p>
                
                <div class="articles-grid">
                    <!-- Article 1 -->
                    <article class="article-card">
                        <div class="article-icon">
                            <i class="fas fa-robot"></i>
                        </div>
                        <h3>The Rise and Potential of Large Language Model-Based Agents: A Survey</h3>
                        <p class="article-overview">Explores how large language models (LLMs) like GPT-3 and beyond can be the foundation for building AI agents with broad capabilities.</p>
                        <div class="article-insights">
                            <h4>Key Insights:</h4>
                            <ul>
                                <li><strong>Agents evolve:</strong> Initially symbolic and reactive agents laid AI's foundation, but LLMs now enable generalized, adaptable agents capable of reasoning, planning, and collaboration.</li>
                                <li><strong>Capabilities:</strong> LLM-based agents are showcased in single-agent, multi-agent, and human-agent interactive settings.</li>
                                <li><strong>Challenges:</strong> Ethical risks, societal impacts, evaluation of agent abilities, and scaling challenges are highlighted as critical research avenues.</li>
                            </ul>
                        </div>
                        <div class="article-actions">
                            <a href="pdfs/The Rise and Potential of Large Language Model Based Agents a Survey.pdf" class="btn-download" target="_blank">
                                <i class="fas fa-download"></i> Download PDF
                            </a>
                        </div>
                    </article>

                    <!-- Article 2 -->
                    <article class="article-card">
                        <div class="article-icon">
                            <i class="fas fa-language"></i>
                        </div>
                        <h3>Language Models are Few-Shot Learners (GPT-3)</h3>
                        <p class="article-overview">Introduces GPT-3, demonstrating that very large LLMs can perform new language tasks from just a few examples or instructions.</p>
                        <div class="article-insights">
                            <h4>Major Findings:</h4>
                            <ul>
                                <li><strong>Few-shot learning:</strong> Scaling up model size substantially improved the ability to learn new tasks directly from context.</li>
                                <li><strong>Performance:</strong> GPT-3 set new standards in zero-shot and few-shot learning.</li>
                                <li><strong>Societal Note:</strong> The model's generative power raises important ethical concerns regarding misinformation and bias.</li>
                            </ul>
                        </div>
                        <div class="article-actions">
                            <a href="pdfs/Language Models are Few Shot Learners.pdf" class="btn-download" target="_blank">
                                <i class="fas fa-download"></i> Download PDF
                            </a>
                        </div>
                    </article>

                    <!-- Article 3 -->
                    <article class="article-card">
                        <div class="article-icon">
                            <i class="fas fa-eye"></i>
                        </div>
                        <h3>Attention Is All You Need (The Transformer Paper)</h3>
                        <p class="article-overview">Presents the Transformer architecture, removing recurrence entirely in favor of self-attention mechanisms.</p>
                        <div class="article-insights">
                            <h4>Impact:</h4>
                            <ul>
                                <li>Enables more parallelizable and scalable models (like GPT, BERT)</li>
                                <li>Foundational to every major current LLM</li>
                                <li>Demonstrates state-of-the-art performance on translation tasks</li>
                            </ul>
                        </div>
                        <div class="article-actions">
                            <span class="btn-download" style="background: #f3f4f6; color: #6b7280; cursor: not-allowed;">
                                <i class="fas fa-file-plus"></i> PDF Coming Soon
                            </span>
                        </div>
                    </article>

                    <!-- Article 4 -->
                    <article class="article-card">
                        <div class="article-icon">
                            <i class="fas fa-users"></i>
                        </div>
                        <h3>Training Language Models to Follow Instructions with Human Feedback</h3>
                        <p class="article-overview">Discusses techniques to fine-tune AI models using reinforcement learning with human feedback (RLHF).</p>
                        <div class="article-insights">
                            <h4>Significance:</h4>
                            <ul>
                                <li>Results in models that are safer and more aligned with human intent</li>
                                <li>Less likely to produce toxic, biased, or undesired outputs</li>
                                <li>Foundation for ChatGPT and similar instruction-following models</li>
                            </ul>
                        </div>
                        <div class="article-actions">
                            <a href="pdfs/Training Language Modesl to Follow Instructions with Human Feedback.pdf" class="btn-download" target="_blank">
                                <i class="fas fa-download"></i> Download PDF
                            </a>
                        </div>
                    </article>

                    <!-- Article 5 -->
                    <article class="article-card">
                        <div class="article-icon">
                            <i class="fas fa-network-wired"></i>
                        </div>
                        <h3>Switch Transformers: Scaling to Trillion Parameter Models</h3>
                        <p class="article-overview">Introduces a sparse, Mixture-of-Experts approach that enables training models with trillions of parameters efficiently.</p>
                        <div class="article-insights">
                            <h4>Significance:</h4>
                            <ul>
                                <li>Reduces computational cost while expanding capability</li>
                                <li>Makes the next generation of AI models both practical and more powerful</li>
                                <li>Enables efficient scaling beyond traditional dense models</li>
                            </ul>
                        </div>
                        <div class="article-actions">
                            <a href="pdfs/Switch Transformers Scaling to Trillion Parameter Models with Simple and Efficient Sparsity.pdf" class="btn-download" target="_blank">
                                <i class="fas fa-download"></i> Download PDF
                            </a>
                        </div>
                    </article>

                    <!-- Article 6 -->
                    <article class="article-card">
                        <div class="article-icon">
                            <i class="fas fa-microchip"></i>
                        </div>
                        <h3>LLM.int8: 8-bit Matrix Multiplication for Transformers</h3>
                        <p class="article-overview">Describes methods to quantize LLMs for inference using 8-bit precision without loss of performance.</p>
                        <div class="article-insights">
                            <h4>Practical Implication:</h4>
                            <ul>
                                <li>Enables running extremely large models on standard hardware</li>
                                <li>Democratizes access to state-of-the-art AI</li>
                                <li>Significant reduction in memory requirements</li>
                            </ul>
                        </div>
                        <div class="article-actions">
                            <a href="pdfs/LLM int8 8bit Matrix Multiplication for Transformers at Scale.pdf" class="btn-download" target="_blank">
                                <i class="fas fa-download"></i> Download PDF
                            </a>
                        </div>
                    </article>

                    <!-- Article 7 -->
                    <article class="article-card">
                        <div class="article-icon">
                            <i class="fas fa-compress-alt"></i>
                        </div>
                        <h3>DistilBERT: Smaller, Faster, Cheaper, and Lighter</h3>
                        <p class="article-overview">Explores knowledge distillation to create compact versions of large language models.</p>
                        <div class="article-insights">
                            <h4>Overview:</h4>
                            <ul>
                                <li>Makes models more deployable in resource-constrained environments</li>
                                <li>Maintains performance while reducing model size</li>
                                <li>Pioneered efficient model compression techniques</li>
                            </ul>
                        </div>
                        <div class="article-actions">
                            <a href="pdfs/DistilBERT a distilled version of BERT smaller faster cheaper and lighter.pdf" class="btn-download" target="_blank">
                                <i class="fas fa-download"></i> Download PDF
                            </a>
                        </div>
                    </article>

                    <!-- Article 8 -->
                    <article class="article-card">
                        <div class="article-icon">
                            <i class="fas fa-layer-group"></i>
                        </div>
                        <h3>LoRA: Low-Rank Adaptation of Large Language Models</h3>
                        <p class="article-overview">Discusses parameter-efficient fine-tuning where only a few parameters are learned.</p>
                        <div class="article-insights">
                            <h4>Overview:</h4>
                            <ul>
                                <li>Allows large pre-trained models to be adapted quickly and cheaply</li>
                                <li>Enables task-specific customization without full retraining</li>
                                <li>Significant reduction in computational requirements for adaptation</li>
                            </ul>
                        </div>
                        <div class="article-actions">
                            <a href="pdfs/LORA Low Rank Adaption of Large Language Models.pdf" class="btn-download" target="_blank">
                                <i class="fas fa-download"></i> Download PDF
                            </a>
                        </div>
                    </article>

                    <!-- Article 9 -->
                    <article class="article-card">
                        <div class="article-icon">
                            <i class="fas fa-search"></i>
                        </div>
                        <h3>Retrieval-Augmented Generation (RAG) for Knowledge-Intensive NLP Tasks</h3>
                        <p class="article-overview">Combines retrieval mechanisms with generation, enhancing open-domain question answering.</p>
                        <div class="article-insights">
                            <h4>Overview:</h4>
                            <ul>
                                <li>Enhances knowledge-intensive tasks through external retrieval</li>
                                <li>Combines parametric and non-parametric knowledge</li>
                                <li>Improves factual accuracy and reduces hallucinations</li>
                            </ul>
                        </div>
                        <div class="article-actions">
                            <a href="pdfs/Retrieval Augmented Generation for Knowledge Intensive NLP Tasks.pdf" class="btn-download" target="_blank">
                                <i class="fas fa-download"></i> Download PDF
                            </a>
                        </div>
                    </article>
                </div>
            </div>
        </section>

        <!-- PDF Resources Section -->
        <section id="resources" class="resources-section">
            <div class="container">
                <h2 class="section-title">PDF Resources</h2>
                <p class="section-subtitle">Download and explore comprehensive AI research papers</p>
                
                <div class="resources-info">
                    <div class="info-card">
                        <i class="fas fa-file-pdf"></i>
                        <h3>Research Papers Available</h3>
                        <p>All research papers are available for download in the Articles section above. Each article card has a direct download link to the full PDF.</p>
                    </div>
                </div>
            </div>
        </section>

        <!-- Repository Section -->
        <section id="repository" class="repository-section">
            <div class="container">
                <h2 class="section-title">GitHub Repository</h2>
                <div class="repo-card">
                    <div class="repo-info">
                        <h3>AI Information Primer</h3>
                        <p>Access the complete source code, documentation, and additional resources on GitHub. Submit issues, suggestions, or contribute to the project.</p>
                        <a href="https://github.com/cyber3pxVA/ai-information-primer" target="_blank" class="btn btn-primary">
                            <i class="fab fa-github"></i> View on GitHub
                        </a>
                    </div>
                    <div class="repo-stats">
                        <div class="stat">
                            <i class="fas fa-file-pdf"></i>
                            <span>9+ Research Papers</span>
                        </div>
                        <div class="stat">
                            <i class="fas fa-brain"></i>
                            <span>AI Education Focus</span>
                        </div>
                        <div class="stat">
                            <i class="fas fa-users"></i>
                            <span>Healthcare Community</span>
                        </div>
                    </div>
                </div>
            </div>
        </section>

        <!-- Contact Section -->
        <section id="contact" class="contact-section">
            <div class="container">
                <h2 class="section-title">Contact & Support</h2>
                <div class="contact-grid">
                    <div class="contact-card">
                        <i class="fab fa-github"></i>
                        <h3>GitHub Repository</h3>
                        <p>Report issues, suggest improvements, or contribute to the project</p>
                        <a href="https://github.com/cyber3pxVA/ai-information-primer" target="_blank" class="contact-link">View Repository & Issues</a>
                    </div>
                    <div class="contact-card">
                        <i class="fas fa-question-circle"></i>
                        <h3>Documentation</h3>
                        <p>Comprehensive guides and tutorials</p>
                        <a href="#" class="contact-link">View Docs</a>
                    </div>
                    <div class="contact-card">
                        <i class="fas fa-comments"></i>
                        <h3>Community</h3>
                        <p>Join discussions about AI research and development</p>
                        <a href="#" class="contact-link">Join Community</a>
                    </div>
                </div>
            </div>
        </section>
    </main>

    <footer class="footer">
        <div class="container">
            <div class="footer-content">
                <div class="footer-brand">
                    <i class="fas fa-brain"></i>
                    <span>AI Education Hub</span>
                </div>
                <div class="footer-links">
                    <a href="#home">Home</a>
                    <a href="#articles">Articles</a>
                    <a href="#resources">Downloads</a>
                    <a href="https://github.com/cyber3pxVA/ai-information-primer" target="_blank">Repository</a>
                </div>
                <div class="footer-info">
                    <p>&copy; 2025 Specialty Care Medicine - New England Healthcare System. All rights reserved.</p>
                </div>
            </div>
        </div>
    </footer>

    <script src="script.js"></script>
</body>
</html>